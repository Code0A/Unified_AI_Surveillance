# Unified AI Surveillance for Personalized Behavioural & Emotional Prediction  
A multimodal AI system combining **deep visual emotion recognition**, **EEG affect prediction**, and **personalized subject modeling** to detect and understand human emotional states in real time.

---

# ğŸ“Œ Overview  
This repository implements a full research-grade **multimodal affective computing framework** using:

### ğŸ§  Modalities Used:
- **Facial expressions**  
- **EEG emotional states**  
- **Subject identity embeddings (personalization)**  

### ğŸ” The system predicts:
- 7-class emotion classification  
- Continuous valence (1â€“9)  
- Continuous arousal (1â€“9)  
- Personalized emotional response model  

This project is designed for **research**, **behavioural science**, **HCI**, **human-aware systems**, and **emotion tracking** â€” NOT for misuse or surveillance without consent.

---

# ğŸ“‚ Repository Structure

unified-ai-surveillance/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ datasets/
â”‚ â”‚ â”œâ”€â”€ fer_loader.py
â”‚ â”‚ â”œâ”€â”€ deap_loader.py
â”‚ â”‚ â”œâ”€â”€ affectnet_loader.py
â”‚ â”‚ â”œâ”€â”€ emotion_kaggle_loader.py
â”‚ â”‚ â””â”€â”€ unified_dataset.py
â”‚ â”‚
â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â”œâ”€â”€ visual_backbone.py
â”‚ â”‚ â”œâ”€â”€ eeg_backbone.py
â”‚ â”‚ â”œâ”€â”€ fusion_model.py
â”‚ â”‚ â””â”€â”€ personalization.py
â”‚ â”‚
â”‚ â”œâ”€â”€ train/
â”‚ â”‚ â”œâ”€â”€ train_visual_only.py
â”‚ â”‚ â””â”€â”€ train_multimodal.py
â”‚ â”‚
â”‚ â”œâ”€â”€ inference/
â”‚ â”‚ â”œâ”€â”€ realtime_inference.py
â”‚ â”‚ â””â”€â”€ calibration.py
â”‚ â”‚
â”‚ â””â”€â”€ utils/
â”‚ â”œâ”€â”€ metrics.py
â”‚ â””â”€â”€ samplers.py
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ preprocess_faces.py
â”‚ â”œâ”€â”€ preprocess_deap.py
â”‚ â””â”€â”€ preprocess_affectnet.py
â”‚
â””â”€â”€ docs/
â”œâ”€â”€ MODEL_CARD.md
â”œâ”€â”€ ETHICS.md
â”œâ”€â”€ DPIA.md
â””â”€â”€ DATASET_GUIDE.md# ğŸ”§ Features

### ğŸ¥ **Visual Emotion Recognition**
- Uses a ResNet18 backbone  
- Trained on FER2013, AffectNet, Kaggle Emotions  
- Predicts 7 emotions

### ğŸ§  **EEG-based Affective Computing**
- Uses DEAP dataset  
- Predicts valence & arousal  
- 1D CNN for EEG processing  

### ğŸ”„ **Multimodal Fusion Model**
- Combines:
  - image embedding (256-dim)
  - EEG embedding (128-dim)
  - subject embedding (64-dim)
- Predicts:
  - emotion class  
  - valence  
  - arousal  

### ğŸ‘¤ **Personalization Layer**
- Trainable subject embedding for each user  
- Calibration routine lets a new user adapt the model to their own emotion patterns

### ğŸ–¥ **Realtime Emotion Prediction**
- Webcam face detection using MTCNN  
- Live smoothing & prediction confidence  
- On-the-fly user calibration  

---

# ğŸ›  Installation

Make sure Python 3.10+ is installed.

Install dependencies:

```bash
pip install -r requirements.txtğŸ“¦ Dataset Preprocessing

You MUST preprocess datasets before training.

FER2013:
python scripts/preprocess_faces.py \
    --src_folder data/raw/fer2013_images \
    --labels_csv data/raw/fer2013.csv \
    --out_folder data/processed/fer2013

AffectNet:
python scripts/preprocess_affectnet.py \
    --img_folder data/raw/affectnet/images \
    --labels_csv data/raw/affectnet/labels.csv \
    --out_folder data/processed/affectnet

DEAP:
python scripts/preprocess_deap.py \
    --raw_folder data/raw/deap/ \
    --out_folder data/processed/deap/

ğŸ§ª Training
ğŸ‘‰ 1. Train Visual Model Only
python src/train/train_visual_only.py \
    --fer_csv data/processed/fer2013.csv \
    --epochs 10 \
    --batch_size 64 \
    --out_dir outputs/visual

ğŸ‘‰ 2. Train Full Multimodal Fusion Model
python src/train/train_multimodal.py \
    --fer_csv data/processed/fer2013.csv \
    --deap_eeg data/processed/deap/eeg \
    --deap_labels data/processed/deap/labels.csv \
    --epochs 20 \
    --batch_size 32 \
    --out_dir outputs/multimodal

ğŸ–¥ Realtime Inference

Run live detection from webcam:

python src/inference/realtime_inference.py \
    --model_ckpt outputs/multimodal/multimodal_best.pt \
    --device cuda

Controls:
Key	Action
q	Quit
c	Start calibration
k	Capture sample during calibration
ğŸ‘¤ User Calibration Mode

Calibration improves accuracy for new users.

Steps:

Press c during realtime inference

Look directly into the camera

Press k to capture each labeled sample

Enter the emotion label (0â€“6) in the terminal

The system creates:

a new subject embedding

a calibrated model checkpoint

ğŸ“˜ Documentation

All docs are in /docs:

MODEL_CARD.md â€” complete model explanation

ETHICS.md â€” misuse prevention, risk analysis

DPIA.md â€” privacy protection compliance

DATASET_GUIDE.md â€” dataset descriptions + preprocessing

âš ï¸ Limitations

Not medically approved

Can misclassify strong lighting/occlusions

EEG predictions vary depending on hardware

Dataset biases influence accuracy

Should not be used for enforcement, surveillance, HR, or legal systems

ğŸ¤ Contributing

Contributions are welcome!Submit a pull request or open an issue.

ğŸ“œ License

MIT License (recommended for research + open use)

âœ¨ Author

Built by Aradhyea Saroha
With complete architecture design, training pipeline, documentation & inference system.


---

If you want:
âœ… A cleaner minimal README  
or  
âœ… A research-paper formatted README  
or  
âœ… A GitHub Pages site version  

Just tell me **â€œMake README minimal / research style / GitHub Pages styleâ€** and Iâ€™ll generate it.


